{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import learning_curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixing the directory path \n",
    "import os\n",
    "os.chdir(\"C:/Users/RAZER BLADE/Downloads/AI Path/Multi-Class-Classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Datasets/train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.copy()\n",
    "\n",
    "# Drop the age column \n",
    "df = df.drop(['Age'], axis=1)\n",
    "\n",
    "# Replace the numerical value of Hepatomegaly \n",
    "df.loc[df['Hepatomegaly'] == '119.35', 'Hepatomegaly'] = df['Hepatomegaly'].mode()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15000, 17) (15000,)\n"
     ]
    }
   ],
   "source": [
    "Y = df['Status']\n",
    "X = df.drop(['Status', 'id'], axis=1)\n",
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        3\n",
       "1        4\n",
       "2        2\n",
       "3        3\n",
       "4        3\n",
       "        ..\n",
       "14995    3\n",
       "14996    4\n",
       "14997    4\n",
       "14998    4\n",
       "14999    4\n",
       "Name: Stage, Length: 15000, dtype: int32"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Stage'] = df['Stage'].astype('int')\n",
    "df['Stage']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainning Shape (12000, 17) (12000,)\n",
      "Trainning Shape (3000, 17) (3000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train,  X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.2, random_state=8)\n",
    "print('Trainning Shape', X_train.shape, Y_train.shape)\n",
    "print('Trainning Shape', X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Status\n",
       "C     8053\n",
       "D     3634\n",
       "CL     313\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting Variables into Numerical and categorical "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols = X_train.select_dtypes(include=['float', 'int']).columns\n",
    "categorical_cols = X_train.select_dtypes(include=['object']).columns\n",
    "\n",
    "X_train_num = X_train[numerical_cols]\n",
    "X_train_cat = X_train[categorical_cols]\n",
    "\n",
    "X_test_num = X_test[numerical_cols]\n",
    "X_test_cat = X_test[categorical_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Drug', 'Sex', 'Ascites', 'Hepatomegaly', 'Spiders', 'Edema'], dtype='object')"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nan Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing the median and mode \n",
    "def imputation_statistics(df_fill):\n",
    "    \"\"\" impute null values with mode for categorical variables and median for numerical ones\n",
    "    \"\"\"\n",
    "    for col in df_fill.columns:\n",
    "        if df_fill[col].isna().any():\n",
    "            df_fill[col] = df_fill[col].fillna(df_fill[col].mode()[0])\n",
    "    return df_fill\n",
    "#X_train_m = imputation_statistics(X_train)\n",
    "#X_test_m = imputation_statistics(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "\n",
    "from sklearn.impute import IterativeImputer, KNNImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "# Numerical Variables \n",
    "Imp = IterativeImputer(estimator=LinearRegression(), max_iter=10, random_state = 0)\n",
    "Imp.fit(X_train_num)\n",
    "\n",
    "X_train_num_imputed = Imp.transform(X_train_num)\n",
    "X_test_num_imputed = Imp.transform(X_test_num)\n",
    "\n",
    "X_train_num_imputed = pd.DataFrame(X_train_num_imputed)\n",
    "X_train_num_imputed.columns = X_train_num.columns\n",
    "X_test_num_imputed = pd.DataFrame(X_test_num_imputed)\n",
    "X_test_num_imputed.columns = X_test_num.columns\n",
    "\n",
    "# Categorical variables\n",
    "X_train_cat_imputed = imputation_statistics(X_train_cat)\n",
    "X_test_cat_imputed = imputation_statistics(X_test_cat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder,StandardScaler\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "\n",
    "from sklearn.impute import IterativeImputer, KNNImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from src.exception import CustomException\n",
    "from src.logger import logging\n",
    "import os\n",
    "\n",
    "from src.utils import save_object\n",
    "\n",
    "# 1. Create a data transfo config to configure path \n",
    "@dataclass\n",
    "class DataTransformationConfig:\n",
    "    preprocessor_obj_file_path = os.path.join('artifact', \"preprocessor.pkl\")\n",
    "    \n",
    "# Creating Data transfo class \n",
    "class DataTransformation:\n",
    "# Initialise the path config within the init\n",
    "    def __init__(self):\n",
    "        self.data_transformation_config = DataTransformationConfig()\n",
    "    \n",
    "    def imputing():\n",
    "        imputer = IterativeImputer(estimator=LinearRegression(), max_iter=10, random_state = 0)\n",
    "        Imp.fit(X_train_num)\n",
    "        X_train_num_imputed = Imp.transform(X_train_num)\n",
    "        X_test_num_imputed = Imp.transform(X_test_num)\n",
    "\n",
    "        X_train_num_imputed = pd.DataFrame(X_train_num_imputed)\n",
    "        X_train_num_imputed.columns = X_train_num.columns\n",
    "        X_test_num_imputed = pd.DataFrame(X_test_num_imputed)\n",
    "        X_test_num_imputed.columns = X_test_num.columns\n",
    "\n",
    "        def imputation_statistics(df_fill):\n",
    "            for col in df_fill.columns:\n",
    "                if df_fill[col].isna().any():\n",
    "                    df_fill[col] = df_fill[col].fillna(df_fill[col].mode()[0])\n",
    "        return df_fill\n",
    "\n",
    "        X_train_cat_imputed = imputation_statistics(X_train_cat)\n",
    "        X_test_cat_imputed = imputation_statistics(X_test_cat)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def get_data_transformer_object(self):\n",
    "        try:\n",
    "            numerical_columns = ['N_Days', 'Bilirubin', 'Cholesterol', \n",
    "                                 'Albumin', 'Copper', 'Alk_Phos',\n",
    "                                'SGOT', 'Tryglicerides', 'Platelets', \n",
    "                                'Prothrombin', 'Stage']\n",
    "            categorical_columns = ['Drug', 'Sex', 'Ascites', 'Hepatomegaly', 'Spiders', 'Edema']\n",
    "           \n",
    "\n",
    "            num_pipeline = Pipeline(\n",
    "                steps=[\n",
    "                (\"imputer\", IterativeImputer(estimator=LinearRegression(), max_iter=10, random_state = 0)),\n",
    "                (\"scaler\", RobustScaler())\n",
    "\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            cat_pipeline = Pipeline(\n",
    "\n",
    "                steps=[\n",
    "                (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                (\"one_hot_encoder\", OneHotEncoder())\n",
    "                \n",
    "                ]\n",
    "\n",
    "            )\n",
    "\n",
    "            logging.info(f\"Categorical columns: {categorical_columns}\")\n",
    "            logging.info(f\"Numerical columns: {numerical_columns}\")\n",
    "\n",
    "            preprocessor = ColumnTransformer(\n",
    "                [\n",
    "                (\"num_pipeline\",    num_pipeline,   numerical_columns),\n",
    "                (\"cat_pipelines\",   cat_pipeline,  categorical_columns)\n",
    "\n",
    "                ]\n",
    "\n",
    "\n",
    "            )\n",
    "\n",
    "            return preprocessor\n",
    "        \n",
    "        except Exception as e:\n",
    "            raise CustomException(e,sys)\n",
    "        \n",
    "    def initiate_data_transformation(self, train_path, test_path):\n",
    "\n",
    "        try:\n",
    "            # Read the train. test that will come from data_ingestion\n",
    "            train_df = pd.read_csv(train_path)\n",
    "            test_df = pd.read_csv(test_path)\n",
    "            logging.info(\"Read train and test data completed\")\n",
    "\n",
    "            # Creating the preprocessor\n",
    "            preprocessing_obj = self.get_data_transformer_object()\n",
    "            logging.info(\"Obtaining preprocessing object\")\n",
    "\n",
    "            # Preparing the X_train Y_train X_test Y_test\n",
    "            target_column_name = \"Status\"\n",
    "            numerical_columns = ['N_Days', 'Bilirubin', 'Cholesterol', \n",
    "                                 'Albumin', 'Copper', 'Alk_Phos',\n",
    "                                'SGOT', 'Tryglicerides', 'Platelets', \n",
    "                                'Prothrombin', 'Stage']\n",
    "            categorical_columns = ['Drug', 'Sex', 'Ascites', 'Hepatomegaly', 'Spiders', 'Edema']\n",
    "            columns_delete = ['id', 'Age']\n",
    "\n",
    "            # Train    \n",
    "            X_train = train_df.drop(columns=columns_delete + [target_column_name], axis=1)\n",
    "            Y_train = train_df[target_column_name]\n",
    "\n",
    "            # Test \n",
    "            X_test = test_df.drop(columns=columns_delete + [target_column_name], axis=1)\n",
    "            Y_test = test_df[target_column_name]\n",
    "\n",
    "            # SPLITTING Categorical and numerical \n",
    "            numerical_cols = X_train.select_dtypes(include=['float', 'int']).columns\n",
    "            categorical_cols = X_train.select_dtypes(include=['object']).columns\n",
    "            # Train \n",
    "            X_train_num = X_train[numerical_cols]\n",
    "            X_train_cat = X_train[categorical_cols]\n",
    "            # Test \n",
    "            X_test_num = X_test[numerical_cols]\n",
    "            X_test_cat = X_test[categorical_cols]\n",
    "\n",
    "            logging.info(\n",
    "                f\"Applying preprocessing object on training dataframe and testing dataframe.\"\n",
    "            )\n",
    "\n",
    "            # fit the preprocessor and transform train and test data \n",
    "            X_train_ = preprocessing_obj.fit_transform(input_feature_train_df)\n",
    "            X_test_ = preprocessing_obj.transform(input_feature_test_df)\n",
    "\n",
    "            #  concatenate arrays features and target along the second axis (i.e., columns)\n",
    "            train_arr = np.c_[input_feature_train_arr, np.array(target_feature_train_df)]\n",
    "            test_arr = np.c_[input_feature_test_arr, np.array(target_feature_test_df)]\n",
    "\n",
    "            \n",
    "            logging.info(f\"Saving the preprocessing object.\")\n",
    "            save_object(\n",
    "\n",
    "                file_path = self.data_transformation_config.preprocessor_obj_file_path,\n",
    "                obj = preprocessing_obj\n",
    "\n",
    "            )\n",
    "            logging.info(f\"Saved ! preprocessing object.\")\n",
    "\n",
    "            return (\n",
    "                train_arr,\n",
    "                test_arr,\n",
    "                self.data_transformation_config.preprocessor_obj_file_path,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            raise CustomException(e,sys)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "import pandas as pd\n",
    "\n",
    "def encoding_mixed(X_train_cat_imputed, X_test_cat_imputed, onehot_cols, ordinal_cols):\n",
    "    \"\"\"\n",
    "    Encodes categorical data using one-hot encoding for some variables and ordinal encoding for others.\n",
    "\n",
    "    Args:\n",
    "        X_train_cat_imputed: DataFrame containing imputed categorical features of the training set.\n",
    "        X_test_cat_imputed: DataFrame containing imputed categorical features of the test set.\n",
    "        onehot_cols: List of column names to apply one-hot encoding.\n",
    "        ordinal_cols: List of column names to apply ordinal encoding.\n",
    "\n",
    "    Returns:\n",
    "        X_train_encoded: Encoded training set as a DataFrame.\n",
    "        X_test_encoded: Encoded test set as a DataFrame.\n",
    "    \"\"\"\n",
    "    # Define the transformers for one-hot and ordinal encoding\n",
    "    transformers = [\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False), onehot_cols),\n",
    "        ('ordinal', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1), ordinal_cols)\n",
    "    ]\n",
    "\n",
    "    # Create the ColumnTransformer\n",
    "    column_transformer = ColumnTransformer(transformers, remainder='passthrough')\n",
    "\n",
    "    # Fit the transformer on the training set and transform both datasets\n",
    "    X_train_encoded = column_transformer.fit_transform(X_train_cat_imputed)\n",
    "    X_test_encoded = column_transformer.transform(X_test_cat_imputed)\n",
    "\n",
    "    # Get feature names for one-hot encoded columns\n",
    "    onehot_feature_names = column_transformer.named_transformers_['onehot'].get_feature_names_out(onehot_cols)\n",
    "\n",
    "    # Combine the one-hot and ordinal column names\n",
    "    all_columns = list(onehot_feature_names) + ordinal_cols + [\n",
    "        col for col in X_train_cat_imputed.columns if col not in onehot_cols + ordinal_cols\n",
    "    ]\n",
    "\n",
    "    # Convert to DataFrame with proper column names\n",
    "    X_train_encoded = pd.DataFrame(X_train_encoded, columns=all_columns, index=X_train_cat_imputed.index)\n",
    "    X_test_encoded = pd.DataFrame(X_test_encoded, columns=all_columns, index=X_test_cat_imputed.index)\n",
    "\n",
    "    return X_train_encoded, X_test_encoded\n",
    "\n",
    "# Usage\n",
    "onehot_cols = ['Ascites', 'Edema']  # Replace with your one-hot encoded column names\n",
    "ordinal_cols = ['Drug', 'Sex', 'Hepatomegaly','Spiders' ]  # Replace with your ordinal encoded column names\n",
    "\n",
    "X_train_cat_encoded, X_test_cat_encoded = encoding_mixed(X_train_cat_imputed, X_test_cat_imputed, onehot_cols, ordinal_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_encoding(Y):\n",
    "    code = {'C':1, 'D':0, 'CL':2}\n",
    "    Y = Y.map(code)\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = target_encoding(Y_train)\n",
    "Y_test = target_encoding(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Status\n",
       "1    8053\n",
       "0    3634\n",
       "2     313\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "def scaling(X_train, X_test):\n",
    "    X_train_tf = X_train.copy()\n",
    "    X_test_tf = X_test.copy()\n",
    "    numerical_features = [col for col in X_train.select_dtypes('float').columns if col != 'Stage']\n",
    "    #only scale numeric varaibles in this case rather than the dummy variables for categories \n",
    "    rob = RobustScaler()\n",
    "    X_train_tf.loc[:, numerical_features] = rob.fit_transform(X_train_tf.loc[:, numerical_features])\n",
    "    X_test_tf.loc[:, numerical_features] = rob.transform(X_test_tf.loc[:, numerical_features])\n",
    "    return X_train_tf, X_test_tf\n",
    "\n",
    "X_train_tf, X_test_tf = scaling(X_train_num_imputed, X_test_num_imputed) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final merge of preprocessed categorical and numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat \n",
    "X_train_final = pd.concat([X_train_tf.reset_index().drop('index', axis=1), X_train_cat_encoded.reset_index().drop('index', axis=1)], axis=1)\n",
    "X_test_final = pd.concat([X_test_tf.reset_index().drop('index', axis=1), X_test_cat_encoded.reset_index().drop('index', axis=1)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final['Stage'] = X_train_final['Stage'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N_Days</th>\n",
       "      <th>Bilirubin</th>\n",
       "      <th>Cholesterol</th>\n",
       "      <th>Albumin</th>\n",
       "      <th>Copper</th>\n",
       "      <th>Alk_Phos</th>\n",
       "      <th>SGOT</th>\n",
       "      <th>Tryglicerides</th>\n",
       "      <th>Platelets</th>\n",
       "      <th>Prothrombin</th>\n",
       "      <th>...</th>\n",
       "      <th>Ascites_D-penicillamine</th>\n",
       "      <th>Ascites_N</th>\n",
       "      <th>Ascites_Y</th>\n",
       "      <th>Edema_N</th>\n",
       "      <th>Edema_S</th>\n",
       "      <th>Edema_Y</th>\n",
       "      <th>Drug</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Hepatomegaly</th>\n",
       "      <th>Spiders</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.125644</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>0.190400</td>\n",
       "      <td>-0.604167</td>\n",
       "      <td>-0.086568</td>\n",
       "      <td>0.086999</td>\n",
       "      <td>-0.059866</td>\n",
       "      <td>0.243489</td>\n",
       "      <td>0.231405</td>\n",
       "      <td>-0.777778</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.235825</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>-0.179641</td>\n",
       "      <td>-1.187500</td>\n",
       "      <td>0.179954</td>\n",
       "      <td>-0.066618</td>\n",
       "      <td>0.245873</td>\n",
       "      <td>0.236141</td>\n",
       "      <td>-0.933884</td>\n",
       "      <td>-0.777778</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.610825</td>\n",
       "      <td>2.166667</td>\n",
       "      <td>-0.662348</td>\n",
       "      <td>-0.791667</td>\n",
       "      <td>8.901006</td>\n",
       "      <td>7.749653</td>\n",
       "      <td>0.812009</td>\n",
       "      <td>-1.765576</td>\n",
       "      <td>-0.371901</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.567655</td>\n",
       "      <td>-0.083333</td>\n",
       "      <td>-0.781231</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>-0.164399</td>\n",
       "      <td>-0.301297</td>\n",
       "      <td>2.115715</td>\n",
       "      <td>-2.703076</td>\n",
       "      <td>0.735537</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.704897</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>-0.037993</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.178978</td>\n",
       "      <td>-0.197752</td>\n",
       "      <td>0.196841</td>\n",
       "      <td>0.030341</td>\n",
       "      <td>-0.909091</td>\n",
       "      <td>-0.222222</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11995</th>\n",
       "      <td>1.313144</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>-1.375645</td>\n",
       "      <td>0.145833</td>\n",
       "      <td>-0.704565</td>\n",
       "      <td>0.431819</td>\n",
       "      <td>0.113595</td>\n",
       "      <td>2.484424</td>\n",
       "      <td>0.429752</td>\n",
       "      <td>-1.111111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11996</th>\n",
       "      <td>1.452320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.849164</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>-1.056848</td>\n",
       "      <td>-1.094396</td>\n",
       "      <td>-1.394078</td>\n",
       "      <td>-0.703076</td>\n",
       "      <td>-0.198347</td>\n",
       "      <td>1.111111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11997</th>\n",
       "      <td>-0.269330</td>\n",
       "      <td>-0.083333</td>\n",
       "      <td>-0.361900</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>-0.045774</td>\n",
       "      <td>-0.138230</td>\n",
       "      <td>0.034627</td>\n",
       "      <td>-0.098578</td>\n",
       "      <td>-0.652893</td>\n",
       "      <td>-0.222222</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11998</th>\n",
       "      <td>0.329897</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>-0.271732</td>\n",
       "      <td>-0.791667</td>\n",
       "      <td>-0.305312</td>\n",
       "      <td>10.135347</td>\n",
       "      <td>-0.929069</td>\n",
       "      <td>0.296924</td>\n",
       "      <td>2.570248</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11999</th>\n",
       "      <td>-0.226804</td>\n",
       "      <td>-0.250000</td>\n",
       "      <td>0.441565</td>\n",
       "      <td>1.083333</td>\n",
       "      <td>-0.610623</td>\n",
       "      <td>-1.121054</td>\n",
       "      <td>-1.096989</td>\n",
       "      <td>2.484424</td>\n",
       "      <td>1.413223</td>\n",
       "      <td>-0.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12000 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         N_Days  Bilirubin  Cholesterol   Albumin    Copper   Alk_Phos  \\\n",
       "0      0.125644  -0.166667     0.190400 -0.604167 -0.086568   0.086999   \n",
       "1     -0.235825   0.083333    -0.179641 -1.187500  0.179954  -0.066618   \n",
       "2     -0.610825   2.166667    -0.662348 -0.791667  8.901006   7.749653   \n",
       "3      0.567655  -0.083333    -0.781231  0.062500 -0.164399  -0.301297   \n",
       "4     -0.704897   0.750000    -0.037993  0.041667  0.178978  -0.197752   \n",
       "...         ...        ...          ...       ...       ...        ...   \n",
       "11995  1.313144  -0.166667    -1.375645  0.145833 -0.704565   0.431819   \n",
       "11996  1.452320   0.000000    -0.849164  0.937500 -1.056848  -1.094396   \n",
       "11997 -0.269330  -0.083333    -0.361900 -0.375000 -0.045774  -0.138230   \n",
       "11998  0.329897  -0.166667    -0.271732 -0.791667 -0.305312  10.135347   \n",
       "11999 -0.226804  -0.250000     0.441565  1.083333 -0.610623  -1.121054   \n",
       "\n",
       "           SGOT  Tryglicerides  Platelets  Prothrombin  ...  \\\n",
       "0     -0.059866       0.243489   0.231405    -0.777778  ...   \n",
       "1      0.245873       0.236141  -0.933884    -0.777778  ...   \n",
       "2      0.812009      -1.765576  -0.371901     0.555556  ...   \n",
       "3      2.115715      -2.703076   0.735537     0.000000  ...   \n",
       "4      0.196841       0.030341  -0.909091    -0.222222  ...   \n",
       "...         ...            ...        ...          ...  ...   \n",
       "11995  0.113595       2.484424   0.429752    -1.111111  ...   \n",
       "11996 -1.394078      -0.703076  -0.198347     1.111111  ...   \n",
       "11997  0.034627      -0.098578  -0.652893    -0.222222  ...   \n",
       "11998 -0.929069       0.296924   2.570248     0.111111  ...   \n",
       "11999 -1.096989       2.484424   1.413223    -0.666667  ...   \n",
       "\n",
       "       Ascites_D-penicillamine  Ascites_N  Ascites_Y  Edema_N  Edema_S  \\\n",
       "0                          0.0        1.0        0.0      1.0      0.0   \n",
       "1                          0.0        1.0        0.0      1.0      0.0   \n",
       "2                          0.0        1.0        0.0      1.0      0.0   \n",
       "3                          0.0        1.0        0.0      1.0      0.0   \n",
       "4                          0.0        1.0        0.0      1.0      0.0   \n",
       "...                        ...        ...        ...      ...      ...   \n",
       "11995                      0.0        1.0        0.0      1.0      0.0   \n",
       "11996                      0.0        1.0        0.0      1.0      0.0   \n",
       "11997                      0.0        1.0        0.0      1.0      0.0   \n",
       "11998                      0.0        1.0        0.0      1.0      0.0   \n",
       "11999                      0.0        1.0        0.0      1.0      0.0   \n",
       "\n",
       "       Edema_Y  Drug  Sex  Hepatomegaly  Spiders  \n",
       "0          0.0   0.0  0.0           0.0      0.0  \n",
       "1          0.0   0.0  0.0           0.0      0.0  \n",
       "2          0.0   1.0  0.0           1.0      1.0  \n",
       "3          0.0   1.0  0.0           1.0      1.0  \n",
       "4          0.0   0.0  0.0           0.0      0.0  \n",
       "...        ...   ...  ...           ...      ...  \n",
       "11995      0.0   1.0  0.0           0.0      0.0  \n",
       "11996      0.0   0.0  0.0           0.0      0.0  \n",
       "11997      0.0   0.0  0.0           0.0      0.0  \n",
       "11998      0.0   0.0  0.0           1.0      0.0  \n",
       "11999      0.0   1.0  0.0           0.0      0.0  \n",
       "\n",
       "[12000 rows x 21 columns]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log loss function \n",
    "def log_loss(Y_test, y_pred_proba):\n",
    "    \"\"\"\n",
    "    Calcule l'opposé de la log-vraisemblance pour un problème de classification multiclasse.\n",
    "    \n",
    "    :param Y_test: ndarray de shape (N, M), les vérités terrain encodées en one-hot (1 pour la classe vraie, 0 sinon).\n",
    "    :param y_pred: ndarray de shape (N, M), les probabilités prédites pour chaque classe.\n",
    "                   Chaque ligne doit contenir les probabilités non normalisées.\n",
    "    :return: float, la log-vraisemblance négative normalisée.\n",
    "    \"\"\"\n",
    "    # Normaliser les probabilités\n",
    "    y_pred_proba = y_pred_proba / np.sum(y_pred_proba, axis=1, keepdims=True)\n",
    "    \n",
    "    # Éviter les extrêmes du logarithme\n",
    "    epsilon = 1e-15\n",
    "    y_pred_proba = np.clip(y_pred_proba, epsilon, 1 - epsilon)\n",
    "\n",
    "    N = Y_test.shape[0]\n",
    "    M = y_pred_proba.shape[1]\n",
    "    y_one_hot = np.zeros((N, M))\n",
    "    y_one_hot[np.arange(N), Y_test] = 1\n",
    "    \n",
    "    # Calcul de la log-vraisemblance\n",
    "    log_loss = -np.sum(y_one_hot * np.log(y_pred_proba)) / Y_test.shape[0]\n",
    "    \n",
    "    return log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score\n",
    ")\n",
    "\n",
    "def evaluate_model(Y_test, Y_pred, y_pred_proba):\n",
    "\n",
    "    loglosss = log_loss(Y_test, y_pred_proba)\n",
    "    accuracy = accuracy_score(Y_test, Y_pred)\n",
    "    precision = precision_score(Y_test, Y_pred, average='micro')\n",
    "    recall = recall_score(Y_test, Y_pred, average='micro')\n",
    "    f1 = f1_score(Y_test, Y_pred, average='micro')\n",
    "    \n",
    "    return accuracy, precision, recall, f1, loglosss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the models\n",
    "from xgboost import XGBClassifier\n",
    "models = {\n",
    "    'k-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5),  # Default k=5\n",
    "    'Decision Tree': DecisionTreeClassifier(max_depth=None, random_state=42),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42),\n",
    "    'xgboost': XGBClassifier( n_estimators=100, max_depth=200)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainning_models(models, X_train, Y_train, X_test, Y_test):\n",
    "    for i in range(len(list(models))):\n",
    "        model = list(models.values())[i]\n",
    "        model.fit(X_train, Y_train) # Train model\n",
    "\n",
    "        # Make predictions\n",
    "        Y_pred = model.predict(X_test)\n",
    "        y_pred_proba = model.predict_proba(X_test)\n",
    "\n",
    "        # Evaluate Test dataset\n",
    "        accuracy, precision, recall, f1, logloss = evaluate_model(Y_test, Y_pred, y_pred_proba)\n",
    "        print(model)\n",
    "        print(\"Accuracy: {0}\".format(accuracy))\n",
    "        print(\"Precision: {0}\".format(precision))\n",
    "        print(\"Recall: {0}\".format(recall))\n",
    "        print(\"f1: {0}\".format(f1))\n",
    "        print(\"logloss: {0}\".format(logloss))\n",
    "        \n",
    "        # Confusion Matrix\n",
    "        cm = confusion_matrix(Y_test, Y_pred)\n",
    "        plt.figure()\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=list(Y_test.unique()), yticklabels=list(Y_test.unique()))\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('Actual')\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.xticks(rotation=0)\n",
    "        plt.tight_layout()\n",
    "        plt.gca().xaxis.set_label_position('top')  # Move x-axis label to the top\n",
    "        plt.gca().xaxis.tick_top()                # Move x-axis ticks to the top\n",
    "        plt.show()\n",
    "\n",
    "        print('='*35)\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainning_models(models, X_train_final, Y_train.to_numpy(), X_test_final, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tree Based models are the best since our Dataset has a lot of outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhancing model's performance with Data processing techniques "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fit(X_new, Y_train, X_test_final):\n",
    "    model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "    model.fit(X_new, Y_train) # Train model\n",
    "\n",
    "    # Make predictions\n",
    "    Y_pred = model.predict(X_test_final[X_new.columns])\n",
    "    y_pred_proba = model.predict_proba(X_test_final[X_new.columns])\n",
    "\n",
    "    # Evaluate Test dataset\n",
    "    accuracy, precision, recall, f1, logloss = evaluate_model(Y_test, Y_pred, y_pred_proba)\n",
    "    print(\"logloss: {0}\".format(logloss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Status\n",
       "1    8053\n",
       "0    3634\n",
       "2     313\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.40878170455654983\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE \n",
    "smote = SMOTE(sampling_strategy = {1:8053, 2:1000, 0:3634} , random_state = 42) \n",
    "X_smote, y_smote = smote.fit_resample(X_train_final ,Y_train)\n",
    "\n",
    "model_fit(X_smote, y_smote, X_test_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import (\n",
    "    SelectKBest, \n",
    "    chi2, \n",
    "    f_classif, \n",
    "    f_regression,\n",
    "    r_regression,\n",
    "    mutual_info_classif,\n",
    "    mutual_info_regression\n",
    ")\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "kb = SelectKBest(f_classif, k=15)\n",
    "X_new = kb.fit_transform(X_train_final, Y_train)\n",
    "X_new = pd.DataFrame(X_new)\n",
    "X_new.columns = kb.get_feature_names_out()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Status\n",
       "1    8053\n",
       "2    3634\n",
       "0    3634\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Balancing Classes : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
